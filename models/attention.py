"""
Attention Implementations for Hybrid SWA and MLA Transformers.

This module provides 4 attention variants:
- NaiveHybridAttention: Portable inference for Hybrid model (RTX 3090)
- FlexHybridAttention: Optimized training for Hybrid model (H100)  
- NaiveMLAttention: Portable inference for MLA model (RTX 3090)
- FlashMLAttention: Optimized training for MLA model (H100)

The factory function `get_attention()` automatically selects the best
implementation based on hardware and mode.
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple

from .config import ModelConfig
from .rope import RotaryEmbedding, apply_rotary_pos_emb, apply_rotary_pos_emb_single



# Optimized kernels are now built-in via SDPA (torch 2.0+)
# Optimized kernels are now built-in via SDPA (torch 2.0+)
try:
    from torch.nn.attention.flex_attention import flex_attention, create_block_mask
    FLEX_AVAILABLE = True
except ImportError:
    try:
        from torch.nn.attention.flex import flex_attention, create_block_mask
        FLEX_AVAILABLE = True
    except ImportError:
        FLEX_AVAILABLE = False

FLASH_MLA_AVAILABLE = True  # We implemented SDPA version of FlashMLA



# ============================================================================
# HYBRID ATTENTION IMPLEMENTATIONS
# ============================================================================

class NaiveHybridAttention(nn.Module):
    """
    Naive PyTorch implementation of Hybrid Sliding Window + Global attention.
    
    Optimized to use:
    - Cached causal/window masks (avoids CPU loop overhead)
    - Vectorized mask creation
    - Explicit SDPA backend selection
    """
    
    def __init__(self, config: ModelConfig, layer_idx: int):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        self.is_global = config.is_global_layer(layer_idx)
        
        self.n_heads = config.n_heads
        self.head_dim = config.head_dim
        self.window_size = config.window_size
        
        # QKV projection (combined for efficiency)
        self.qkv_proj = nn.Linear(config.d_model, 3 * config.d_model, bias=False)
        self.out_proj = nn.Linear(config.d_model, config.d_model, bias=False)
        
        # RoPE
        self.rope = RotaryEmbedding(self.head_dim, config.block_size, config.rope_base)
        
        # Mark output projection for scaled init
        self.out_proj.RESIDUAL_SCALE_INIT = True
        
        # Cache mask for training (fixed block size)
        self.register_buffer("mask_cache", None, persistent=False)
        
    def forward(
        self,
        x: torch.Tensor,
        position_ids: Optional[torch.Tensor] = None,
        kv_cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        use_cache: bool = False,
    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:
        """
        Args:
            x: Input tensor (batch, seq_len, d_model)
            position_ids: Position indices for RoPE
            kv_cache: Cached (keys, values) for generation
            use_cache: Whether to return updated cache
            
        Returns:
            output: Attention output (batch, seq_len, d_model)
            new_cache: Updated KV cache if use_cache=True
        """
        B, S, D = x.shape
        
        # Project to QKV
        qkv = self.qkv_proj(x)
        q, k, v = qkv.chunk(3, dim=-1)
        
        # Reshape for multi-head attention: (B, S, D) -> (B, nh, S, hd)
        q = q.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)
        k = k.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)
        v = v.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)
        
        # Apply RoPE
        q, k = self.rope(q, k, position_ids)
        
        # Handle KV cache for generation
        # CRITICAL: For local (SWA) layers, we implement a ROLLING BUFFER
        if kv_cache is not None:
            past_k, past_v = kv_cache
            k = torch.cat([past_k, k], dim=2)
            v = torch.cat([past_v, v], dim=2)
        
        if use_cache:
            if not self.is_global:
                # LOCAL LAYER: Cap cache at window_size (rolling buffer)
                if k.shape[2] > self.window_size:
                    k_cache = k[:, :, -self.window_size:, :]
                    v_cache = v[:, :, -self.window_size:, :]
                else:
                    k_cache = k
                    v_cache = v
                new_cache = (k_cache, v_cache)
            else:
                # GLOBAL LAYER: Keep full cache
                new_cache = (k, v)
        else:
            new_cache = None
        
        # Attention Logic
        if self.is_global:
            # Global: Standard causal SDPA
            # We prefer FLASH_ATTENTION for speed on Ampere+
            with torch.nn.attention.sdpa_kernel([torch.nn.attention.SDPBackend.FLASH_ATTENTION, torch.nn.attention.SDPBackend.EFFICIENT_ATTENTION, torch.nn.attention.SDPBackend.MATH]):
                out = F.scaled_dot_product_attention(q, k, v, is_causal=True)
        else:
            # Sliding Window
            kv_len = k.shape[2]
            q_len = q.shape[2]
            
            # Efficiently get or create mask
            mask = self._get_sliding_window_mask(q_len, kv_len, x.device)
            
            # Use SDPA with explicit mask
            # Note: FlashAttention 2 supports slight window attention via specialized kernels,
            # but standard sdp_kernel might fall back to efficient_attention or math if mask is dense-ish.
            with torch.nn.attention.sdpa_kernel([torch.nn.attention.SDPBackend.FLASH_ATTENTION, torch.nn.attention.SDPBackend.EFFICIENT_ATTENTION, torch.nn.attention.SDPBackend.MATH]):
                out = F.scaled_dot_product_attention(q, k, v, attn_mask=mask)
        
        # Reshape and project output
        out = out.transpose(1, 2).contiguous().view(B, S, D)
        out = self.out_proj(out)
        
        return out, new_cache
    
    def _get_sliding_window_mask(
        self, q_len: int, kv_len: int, device: torch.device
    ) -> torch.Tensor:
        """
        Get cached mask or create one efficiently using vectorized ops.
        Avoids slow Python loops.
        """
        # 1. Check if we can reuse the cached mask (Training scenario)
        if self.mask_cache is not None and \
           self.mask_cache.shape == (q_len, kv_len) and \
           self.mask_cache.device == device:
            return self.mask_cache
            
        # 2. Vectorized mask creation
        # Indices: (Q, 1) - (1, KV) gives relative distance
        # q_idx[i] = i (if q is full seq) or offset+i (if q is chunk)
        # But for standard forward passes, q is aligned at end of kv usually?
        
        # Assumption: In standard causal attention (train or inference):
        # The query tokens Q[0..q_len] align with Keys K[kv_len-q_len .. kv_len]
        # i.e. the last q_len keys are the ones matching Q
        
        # Construct absolute positions
        # KV indices: 0, 1, ..., kv_len-1
        # Q indices:  kv_len-q_len, ..., kv_len-1
        
        kv_indices = torch.arange(kv_len, device=device).unsqueeze(0)  # (1, KV)
        q_indices = torch.arange(kv_len - q_len, kv_len, device=device).unsqueeze(1) # (Q, 1)
        
        diff = q_indices - kv_indices
        
        # Mask conditions:
        # 1. Causal: q >= k (diff >= 0)
        # 2. Window: q - k < window (diff < window)
        # Valid: 0 <= diff < window
        
        # Create mask initialized to -inf
        mask = torch.full((q_len, kv_len), float("-inf"), device=device)
        
        # Set valid positions to 0.0
        # This is VASTLY faster than a python loop for 2048x2048
        valid_mask = (diff >= 0) & (diff < self.window_size)
        mask.masked_fill_(valid_mask, 0.0)
        
        # Cache it if it matches block size (typical training case)
        if q_len == self.config.block_size and kv_len == self.config.block_size:
            self.mask_cache = mask
            
        return mask


class FlexHybridAttention(nn.Module):
    """
    Optimized Hybrid attention using torch flex_attention API.
    
    Requires H100/A100 with PyTorch 2.5+ and compiled kernels.
    Falls back to NaiveHybridAttention if flex_attention unavailable.
    """
    
    def __init__(self, config: ModelConfig, layer_idx: int):
        super().__init__()
        
        if not FLEX_AVAILABLE:
            raise RuntimeError("flex_attention not available. Use NaiveHybridAttention.")
            
        self.config = config
        self.layer_idx = layer_idx
        self.is_global = config.is_global_layer(layer_idx)
        
        self.n_heads = config.n_heads
        self.head_dim = config.head_dim
        self.window_size = config.window_size
        self.block_size = config.block_size  # Fixed training sequence length
        
        self.qkv_proj = nn.Linear(config.d_model, 3 * config.d_model, bias=False)
        self.out_proj = nn.Linear(config.d_model, config.d_model, bias=False)
        
        self.rope = RotaryEmbedding(self.head_dim, config.block_size, config.rope_base)
        self.out_proj.RESIDUAL_SCALE_INIT = True
        
        # Pre-compute the block mask for training (fixed sequence length)
        # This is the critical optimization - create_block_mask is expensive!
        self._cached_block_mask = None
        if not self.is_global:
            self._block_mask_fn = self._create_sliding_window_mask_fn()
            # Pre-create for training block size (will be populated on first forward)
    
    def _create_sliding_window_mask_fn(self):
        """Create a mask function for flex_attention."""
        window = self.window_size
        
        def sliding_window_mask(b, h, q_idx, kv_idx):
            # Causal constraint: q_idx >= kv_idx
            # Window constraint: q_idx - kv_idx < window_size
            return (q_idx >= kv_idx) & (q_idx - kv_idx < window)
        
        return sliding_window_mask
    
    def _get_block_mask(self, seq_len: int, device: torch.device):
        """Get cached block mask or create one for the given sequence length."""
        # Check if we can reuse cached mask (same seq_len)
        if (self._cached_block_mask is not None and 
            self._cached_block_mask.shape[-1] == seq_len):
            return self._cached_block_mask
        
        # Create new block mask (expensive operation - should only happen once per unique seq_len)
        block_mask = create_block_mask(
            self._block_mask_fn,
            B=None, H=None,
            Q_LEN=seq_len, KV_LEN=seq_len,
            device=device
        )
        
        # Cache it for training (fixed block_size)
        if seq_len == self.block_size:
            self._cached_block_mask = block_mask
            
        return block_mask

    
    def forward(
        self,
        x: torch.Tensor,
        position_ids: Optional[torch.Tensor] = None,
        kv_cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        use_cache: bool = False,
    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:
        B, S, D = x.shape
        
        qkv = self.qkv_proj(x)
        q, k, v = qkv.chunk(3, dim=-1)
        
        q = q.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)
        k = k.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)
        v = v.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)
        
        q, k = self.rope(q, k, position_ids)
        
        # Note: flex_attention is primarily for training (no KV cache)
        # For generation, fall back to naive with rolling buffer
        if kv_cache is not None or use_cache:
            # Fallback to standard SDPA for generation
            if kv_cache is not None:
                past_k, past_v = kv_cache
                k = torch.cat([past_k, k], dim=2)
                v = torch.cat([past_v, v], dim=2)
            
            # Implement rolling buffer for local layers
            if use_cache:
                if not self.is_global:
                    # LOCAL LAYER: Cap cache at window_size
                    if k.shape[2] > self.window_size:
                        k_cache = k[:, :, -self.window_size:, :]
                        v_cache = v[:, :, -self.window_size:, :]
                    else:
                        k_cache = k
                        v_cache = v
                    new_cache = (k_cache, v_cache)
                else:
                    # GLOBAL LAYER: Keep full cache
                    new_cache = (k, v)
            else:
                new_cache = None
            
            if self.is_global:
                out = F.scaled_dot_product_attention(q, k, v, is_causal=True)
            else:
                kv_len = k.shape[2]
                mask = self._make_sliding_window_mask_fallback(S, kv_len, x.device)
                out = F.scaled_dot_product_attention(q, k, v, attn_mask=mask)
        else:
            # Training path: use flex_attention
            new_cache = None
            
            if self.is_global:
                # Full causal attention
                out = F.scaled_dot_product_attention(q, k, v, is_causal=True)
            else:
                # Use flex_attention with CACHED block mask (critical for performance)
                block_mask = self._get_block_mask(S, x.device)
                out = flex_attention(q, k, v, block_mask=block_mask)
        
        out = out.transpose(1, 2).contiguous().view(B, S, D)
        out = self.out_proj(out)
        
        return out, new_cache
    
    def _make_sliding_window_mask_fallback(self, q_len, kv_len, device):
        """Fallback mask for generation mode."""
        mask = torch.full((q_len, kv_len), float("-inf"), device=device)
        for i in range(q_len):
            abs_pos = kv_len - q_len + i
            start = max(0, abs_pos - self.window_size + 1)
            end = abs_pos + 1
            mask[i, start:end] = 0.0
        return mask


# ============================================================================
# MLA (Multi-Head Latent Attention) IMPLEMENTATIONS
# ============================================================================

class NaiveMLAttention(nn.Module):
    """
    Naive PyTorch implementation of Multi-Head Latent Attention.
    
    Implements the DeepSeek-V2/V3 MLA mechanism with:
    - Low-rank KV compression into latent vector c_KV
    - Decoupled RoPE for positional information
    - Standard matmul operations (no FlashMLA kernel)
    
    Works on any hardware (RTX 3090, CPU, etc.).
    """
    
    def __init__(self, config: ModelConfig, layer_idx: int):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        
        self.n_heads = config.n_heads
        self.head_dim = config.head_dim
        self.d_model = config.d_model
        self.kv_lora_rank = config.kv_lora_rank  # d_c (latent dimension)
        self.rope_dim = config.rope_dim           # d_R (RoPE dimension)
        
        # Query projection (full dimension)
        self.q_proj = nn.Linear(config.d_model, config.n_heads * config.head_dim, bias=False)
        
        # Decoupled Query RoPE projection
        self.q_rope_proj = nn.Linear(config.d_model, config.n_heads * self.rope_dim, bias=False)
        
        # KV down-projection to latent space
        self.kv_down_proj = nn.Linear(config.d_model, self.kv_lora_rank, bias=False)
        
        # KV up-projection from latent space (Fused for speed)
        self.kv_up_proj = nn.Linear(self.kv_lora_rank, 2 * config.n_heads * config.head_dim, bias=False)

        
        # Decoupled Key RoPE projection (shared across heads in DeepSeek style)
        self.k_rope_proj = nn.Linear(config.d_model, self.rope_dim, bias=False)
        
        # Output projection
        self.out_proj = nn.Linear(config.d_model, config.d_model, bias=False)
        self.out_proj.RESIDUAL_SCALE_INIT = True
        
        # RoPE for the decoupled positional embeddings
        self.rope_content = RotaryEmbedding(config.head_dim, config.block_size, config.rope_base)
        self.rope_decoupled = RotaryEmbedding(self.rope_dim, config.block_size, config.rope_base)
        
    def forward(
        self,
        x: torch.Tensor,
        position_ids: Optional[torch.Tensor] = None,
        kv_cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        use_cache: bool = False,
    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:
        """
        Args:
            x: Input (batch, seq_len, d_model)
            kv_cache: Cached (c_KV, k_rope) for generation
            
        Returns:
            output, new_cache
        """
        B, S, D = x.shape
        
        # === Query Path ===
        # Content query
        q_content = self.q_proj(x)  # (B, S, n_heads * head_dim)
        q_content = q_content.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)
        
        # Decoupled RoPE query
        q_rope = self.q_rope_proj(x)  # (B, S, n_heads * rope_dim)
        q_rope = q_rope.view(B, S, self.n_heads, self.rope_dim).transpose(1, 2)
        q_rope = self.rope_decoupled.forward_single(q_rope, position_ids)
        
        # === Key-Value Path ===
        # Compress to latent space
        c_kv = self.kv_down_proj(x)  # (B, S, kv_lora_rank)
        
        # Decoupled RoPE key (shared across heads)
        k_rope = self.k_rope_proj(x)  # (B, S, rope_dim)
        k_rope = k_rope.unsqueeze(2)  # (B, S, 1, rope_dim) for broadcasting
        k_rope = k_rope.transpose(1, 2)  # (B, 1, S, rope_dim)
        k_rope = self.rope_decoupled.forward_single(k_rope, position_ids)
        k_rope = k_rope.expand(-1, self.n_heads, -1, -1)  # (B, n_heads, S, rope_dim)
        
        # Handle KV cache
        # In MLA, we cache the compressed c_kv and the RoPE key
        if kv_cache is not None:
            past_c_kv, past_k_rope = kv_cache
            c_kv = torch.cat([past_c_kv, c_kv], dim=1)
            k_rope = torch.cat([past_k_rope, k_rope], dim=2)
        
        new_cache = (c_kv, k_rope) if use_cache else None
        
        # Up-project keys and values from latent space (chunking the fused projection)
        kv_content = self.kv_up_proj(c_kv) # (B, S_kv, 2 * n_heads * head_dim)
        k_content, v = kv_content.chunk(2, dim=-1)
        
        k_content = k_content.view(B, -1, self.n_heads, self.head_dim).transpose(1, 2)
        v = v.view(B, -1, self.n_heads, self.head_dim).transpose(1, 2)

        
        # === Attention Computation ===
        # Concatenate content and RoPE dimensions for query and key
        # q_full: (B, n_heads, S_q, head_dim + rope_dim)
        # k_full: (B, n_heads, S_kv, head_dim + rope_dim)
        q_full = torch.cat([q_content, q_rope], dim=-1)
        k_full = torch.cat([k_content, k_rope], dim=-1)
        
        # Compute attention scores
        scale = 1.0 / math.sqrt(self.head_dim + self.rope_dim)
        attn_weights = torch.matmul(q_full, k_full.transpose(-1, -2)) * scale
        
        # Apply causal mask
        S_q = q_full.shape[2]
        S_kv = k_full.shape[2]
        causal_mask = torch.triu(
            torch.full((S_q, S_kv), float("-inf"), device=x.device), 
            diagonal=S_kv - S_q + 1
        )
        attn_weights = attn_weights + causal_mask
        
        # Softmax and apply to values
        attn_weights = F.softmax(attn_weights, dim=-1)
        out = torch.matmul(attn_weights, v)
        
        # Reshape and project output
        out = out.transpose(1, 2).contiguous().view(B, S, D)
        out = self.out_proj(out)
        
        return out, new_cache


class FlashMLAttention(nn.Module):
    """
    Optimized MLA using FlashMLA kernel (requires H100/H800).
    
    Falls back to NaiveMLAttention if flash_mla is not available.
    """
    
    def __init__(self, config: ModelConfig, layer_idx: int):
        super().__init__()
        
        if not FLASH_MLA_AVAILABLE:
            # Should not happen with SDPA implementation
            raise RuntimeError("FlashMLA optimization not available.")
        
        # For now, just wrap naive implementation
        # Full FlashMLA integration requires the flash_mla package
        self.naive_impl = NaiveMLAttention(config, layer_idx)
        
    def forward(
        self,
        x: torch.Tensor,
        position_ids: Optional[torch.Tensor] = None,
        kv_cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        use_cache: bool = False,
    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:
        """
        Optimized forward pass using SDPA (FlashAttention-2/3).
        
        This implementation is for TRAINING. It up-projects the latent KV
        into full tensors to utilize the highly optimized FlashAttention kernels.
        
        Fairness Note: This produces the exact same mathematical result as 
        NaiveMLAttention but uses hardware-accelerated kernels for O(S^2) ops.
        """
        # Inference fallback: if using cache, use the naive implementation 
        # which is already optimized for KV cache memory.
        if kv_cache is not None or use_cache:
            return self.naive_impl(x, position_ids, kv_cache, use_cache)
            
        B, S, D = x.shape
        impl = self.naive_impl
        
        # 1. Query Projections
        q_content = impl.q_proj(x).view(B, S, impl.n_heads, impl.head_dim).transpose(1, 2)
        q_rope = impl.q_rope_proj(x).view(B, S, impl.n_heads, impl.rope_dim).transpose(1, 2)
        q_rope = impl.rope_decoupled.forward_single(q_rope, position_ids)
        
        # 2. Key-Value Projections (Training pass: up-project full sequence)
        c_kv = impl.kv_down_proj(x)
        
        # Decoupled RoPE key (shared across heads)
        k_rope = impl.k_rope_proj(x).unsqueeze(2).transpose(1, 2) # (B, 1, S, d_R)
        k_rope = impl.rope_decoupled.forward_single(k_rope, position_ids)
        k_rope = k_rope.expand(-1, impl.n_heads, -1, -1) # (B, nh, S, d_R)
        
        # Up-project content keys and values (fused)
        kv_content = impl.kv_up_proj(c_kv).view(B, S, impl.n_heads, 2 * impl.head_dim).transpose(1, 2)
        k_content, v = kv_content.chunk(2, dim=-1) # (B, nh, S, d_h)

        
        # 3. Concatenate Content and RoPE
        # q_full: (B, nh, S, d_h + d_R)
        # k_full: (B, nh, S, d_h + d_R)
        q_full = torch.cat([q_content, q_rope], dim=-1)
        k_full = torch.cat([k_content, k_rope], dim=-1)
        
        # 4. SDPA (This triggers FlashAttention-2/3 on H100)
        # DeepSeek scaling: 1 / sqrt(head_dim + rope_dim)
        # SDPA uses 1 / sqrt(last_dim) by default, which is exactly (head_dim + rope_dim)
        with torch.nn.attention.sdpa_kernel([torch.nn.attention.SDPBackend.FLASH_ATTENTION, torch.nn.attention.SDPBackend.EFFICIENT_ATTENTION]):
            out = F.scaled_dot_product_attention(
                q_full, k_full, v, 
                is_causal=True
            )
        
        # 5. Output Project
        out = out.transpose(1, 2).contiguous().view(B, S, D)
        out = impl.out_proj(out)
        
        return out, None


# ============================================================================
# FACTORY FUNCTION
# ============================================================================

def get_attention(
    config: ModelConfig, 
    layer_idx: int, 
    mode: str = "train"
) -> nn.Module:
    """
    Factory function to get the appropriate attention implementation.
    
    Args:
        config: Model configuration
        layer_idx: Layer index (for Hybrid layer type selection)
        mode: "train" (use optimized kernels) or "inference" (use naive)
        
    Returns:
        Attention module instance
    """
    attn_cls = None
    
    if config.model_type == "hybrid":
        if mode == "train" and FLEX_AVAILABLE:
            attn_cls = FlexHybridAttention
        else:
            attn_cls = NaiveHybridAttention
    
    elif config.model_type == "mla":
        if mode == "train" and FLASH_MLA_AVAILABLE:
            attn_cls = FlashMLAttention
        else:
            attn_cls = NaiveMLAttention
    
    else:
        raise ValueError(f"Unknown model type: {config.model_type}")
        
    # Debug log for first layer
    if layer_idx == 0:
        print(f"Layer 0 Attention: {attn_cls.__name__} (mode={mode}, flex_avail={FLEX_AVAILABLE})")
        
    return attn_cls(config, layer_idx)
